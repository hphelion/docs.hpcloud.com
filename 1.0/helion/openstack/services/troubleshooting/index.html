<!doctype html>
<!--[if lt IE 7]> <html class="no-js ie6 oldie" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js ie7 oldie" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js ie8 oldie" lang="en"> <![endif]-->
<!--[if IE 9]>    <html class="no-js ie9" lang="en"> <![endif]-->
<!--[if gt IE 9]><!--> <html class="no-js" lang="en"> <!--<![endif]-->

<!-- Mirrored from docs.hpcloud.com/helion/openstack/services/troubleshooting/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 20 Feb 2015 14:52:29 GMT -->
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

	<title>HP Helion OpenStack&#174; Orchestration High Availability Service Overview</title>
	
	
	<meta name="viewport" content="width=device-width,initial-scale=1">
	<link rel="shortcut icon" href="../../../../favicon.ico" type="image/vnd.microsoft.icon" />
  <link rel="stylesheet" href="../../../../css/combined.css">

	<script src="../../../../js/modernizr-2.5.3.min.js"></script>
	<script src="../../../../js/metrics.js"></script> 
</head>
<body>
  <div id="wrapper">
    <!--[if lt IE 7]>
    	<p class="chromeframe">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
    <![endif]-->
    <div class="header-container">
      <header class="header">
      	<div class="top-bar" style="height: 75px;">
		
      		<div class="search-wrapper wrapper container_16">
			
      			<a href="#" class="sales-rep show-320 show-480 hide-768"><span aria-hidden="true" class="icon-icon-sales-rep"></span></a>
      			<nav class="sub-nav" >
				<a href="../../../../index.html" class="logo"  ><img src="../../../../images/HP_Helion_logo_72.png" style="position: absolute;top: 10px;" alt="HP Helion" /></a>
        			<ul>
					
 

         
        				<li><a href="../../../../../external.html?link=http://www8.hp.com/us/en/cloud/helion-portfolio.html">Helion@HP</a></li>
						<li><a href="../../../../../external.html?link=https://h30580.www3.hp.com/">Support</a></li>
        				<li><a href="mailto:heliondocs@hp.com">Contact</a></li>
						
        				 
        			</ul>
      			</nav>
          </div>
      	</div>
		<!-- per JC
      	<div class="nav-wrapper wrapper container_16">
        	<a href="http://www.hpcloud.com/" class="logo"><img src="/images/HP_Helion_logo_72.png" alt="HP Helion" /></a>
      		<nav class="nav hide-320 show-768">
      			<ul>
      				<li><a href="http://hpcloud.com/products-services">Products <span>& Services</span></a></li>
      				<li><a href="http://hpcloud.com/solutions">Cloud <span>Solutions</span></a></li>
      				<li><a href="http://hpcloud.com/why-hp-cloud">Why HP <span>Cloud?</span></a></li>
      				<li><a href="http://hpcloud.com/learning-center">Learning <span>Center</span></a></li>
      				<li><a href="http://hpcloud.com/blog">The <span>Blog</span></a></li>
      				<li><a href="http://hpcloud.com/free-trial">Sign Up <span>Today</span></a></li>
      			</ul>
      		</nav>
      	</div>
		-->
      </header>
    </div>
		<div id="main-container">
			<div id="main" class="wrapper clearfix container_16">
				<div id="content" class="grid_10 suffix_1">
	<article>
		<!--PUBLISHED-->

<script>
<![CDATA[

function PageRefresh {
onLoad="window.refresh"
}

PageRefresh();

]]>
</script><!--

<p style="font-size: small;"> <a href="/helion/openstack/services/object/overview/">&#9664; PREV</a> | <a href="/helion/openstack/services/overview/">&#9650; UP</a> | <a href="/helion/openstack/services/reporting/overview/"> NEXT &#9654</a> </p> ---><h1 id="hp-helion-openstack-10--troubleshooting">HP Helion OpenStack® 1.0  Troubleshooting</h1>

<p>HP Helion OpenStack® is an OpenStack technology coupled with a version of Linux® provided by HP. This topic describes all the known issues that you might encounter. To help you resolve these issues, we have provided possible solutions.</p>

<p>For easy reference, we categorized the known issues and solutions as follows:</p>

<ul><li><a href="#baremetal-install">Baremetal installation</a>

<ul><li><a href="#kvm">KVM</a></li>
<li><a href="#esx-ovsvapp">ESX and OVSvAPP</a></li>
</ul></li>
<li><a href="#vsa">VSA</a></li>
<li><a href="#failnew">Failure of newly added compute or VSA node during Scale-out</a></li>
<li><a href="#refreshfails">Scale-out nodes : os-refresh-configuration fails on Controller Node</a></li>
<li><a href="#config_dnas">Configuring the dnsmasq_dns_servers list for the undercloud and overcloud</a></li>
<li><a href="#logging">Logging</a></li>
</ul><p>If you need further assistance, contact <a href="../../../../../external.html?link=http://www.hpcloud.com/about/contact">HP Customer Support</a>.</p>

<h2 id="baremetal-install">Baremetal installation</h2>

<h3 id="kvm">KVM</h3>

<ol><li><a href="#fatal-pci">Fatal PCI Express Device Error</a></li>
<li><a href="#IPMI-fails">IPMI fails with error- unable to establish IPMI v2 / RMCP+ session</a></li>
<li><a href="#failure-update-overcloud">Failure of Update overcloud</a></li>
<li><a href="#installation-failure">Installation failure as the flavor to be used for overcloud nodes does not match</a></li>
<li><a href="#PXE-boot-on-target">PXE boot on target node keeps switching between interfaces</a></li>
<li><a href="#BIOS-blocks-are-not-set-to-correct-date">BIOS blocks are not set to correct date and time across all nodes</a></li>
<li><a href="#ilo-console">iLO console shows hLinux daemon.err tgtd while PXE booting</a></li>
<li><a href="#ilo-show-null">iLO console shows null waiting for notice of completion while PXE booting</a></li>
<li><a href="#failure-installer">Failure of Hp_ced_installer.sh</a></li>
<li><a href="#seed-install-failure">Failure of Seed Installation</a></li>
<li><a href="#ironic">Ironic intermitently set maintenance mode to True during update</a></li>
</ol><h3 id="fatal-pci">Fatal PCI Express Device Error</h3>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>When installing on HP ProLiant SL390s and HP ProLiant BL490d systems, the following error has occasionally occurred:</p>

<pre>
  <code>`Fatal PCI Express Device Error PCI Slot ?
 B00/D00/F00`
</code>
</pre>

<p>
  <strong>Resolution</strong>
</p>

<p>If you get this error, reset the system that experienced the error:</p>

<ol><li>Connect to the iLO using Internet Explorer:
    <code>https://&lt;iLO IP address&gt;</code></li>
<li>Navigate to Information / Diagnostics.</li>
<li>Reset iLO.</li>
<li>Log back into the iLO after 30 seconds.</li>
<li>Navigate to Remote Console / Remote Console.</li>
<li>Open the integrated remote console (.NET).</li>
<li>Click Power switch / Press and Hold.</li>
<li><p>Click Power switch / Momentary Press, and wait for the system to restart.</p>

<p>The system should now boot normally.</p></li>
</ol><ul><li><p>If the overcloud controller is rebooted (due to a power issue, hardware upgrade, or similar event), OpenStack compute tools such as <code>nova-list</code> might report that the VMs are in an ERROR state, rendering the overcloud unusable. To restore the overcloud to an operational state, follow the steps below:</p>

<ol><li><p>As user <code>root</code> on the overcloud controller you must:</p>

<p>A. Run the <code>os-refresh-config</code> scripts:</p>

<pre><code># os-refresh-config
</code></pre>

<p>B. Restart the <code>mysql</code> service:</p>

<pre><code># service mysql restart
</code></pre>

<p>C. Re-run the <code>os-refresh-config</code> scripts:</p>

<pre><code># os-refresh-config
</code></pre>

<p>D. Restart all Networking Operations (Neutron) services:</p>

<pre><code># service neutron-dhcp-agent restart
# service neutron-l3-agent restart
# service neutron-metadata-agent restart
# service neutron-openvswitch-agent restart
# service neutron-server restart
</code></pre></li>
<li><p>On each overcloud node, restart the Neutron and Nova services:</p>

<pre><code>$ sudo service neutron-openvswitch-agent restart
$ sudo service nova-compute restart
$ sudo service nova-scheduler restart
$ sudo service nova-conductor restart
</code></pre></li>
</ol></li>
<li><p>The installer uses IPMI commands to reset nodes and change their power status. Some systems change to a state in which the <code>Server Power</code> status as reported by the iLO is stuck in <code>RESET</code>. If this occurs, you must physically disconnect the power from the server for 10 seconds. If the problem persists after that, contact HP Support as there might be a defective component in the system.</p></li>
<li><p>On the system on which the installer is run, the seed VM's networking is bridged onto the external LAN. If you remove HP Helion OpenStack, the network bridge persists. To revert the network configuration to its pre-installation state, run the following commands as user root:</p>

<pre><code># ip addr add 192.168.185.131/16 dev eth0 scope global
# ip addr del 192.168.185.131/16 dev brbm
# ovs-vsctl del-port NIC
</code></pre>

<p>where</p>

<ul><li>eth0 is the external interface     </li>
<li>192.168.185.131 is the IP address on the external interface - you should replace this with your own IP address.</li>
<li>The baremetal bridge is always called 'brbm'</li>
</ul></li>
<li><p>Before you install the HP Helion OpenStack DNSaaS or if you want to use Heat with HP Helion OpenStack, you <strong>must</strong> modify the /etc/heat/heat.conf file on the overcloud controller as follows.</p>

<p><strong>Important</strong>: The installation of the HP Helion OpenStack DNSaaS fails if you do not make these modifications.</p>

<ol><li><p>Make sure the IP address in the following settings reflects the IP address of the overcloud controller, for example:</p>

<pre><code>heat_metadata_server_url = http://192.0.202.2:8000
heat_waitcondition_server_url = http://192.0.202.2:8000/v1/waitcondition
heat_watch_server_url = http://192.0.202.2:8003
</code></pre>

<p><strong>Note</strong>: You must have admin ssh access to the overcloud controller.</p></li>
<li><p>Save the file.</p></li>
<li><p>Restart the Heat-related services – heat-api, heat-api-cfn, heat-api-cloudwatch, and heat-engine.</p></li>
<li><p>Ensure there are no Heat resources in an error state, and then delete any stale or corrupted Heat-related stacks.
<br /><br /></p></li>
</ol></li>
</ul><hr /><h3 id="IPMI-fails">IPMI fails with an error- unable to establish IPMI v2 / RMCP+ session</h3>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>When installing on HP ProLiant BL490c systems, the following error has occasionally occurred:</p>

<pre>
  <code>unable to establish IPMI v2 / RMCP+ session
</code>
</pre>

<p>
  <strong>Resolution</strong>
</p>

<p>If you get this error, perform the following steps:</p>

<ol><li>Ensure that the iLO user has administrator privileges, which is required by the IPMITOOL.</li>
<li>To check from the iLO remote console, reboot the server and press <strong>F8</strong> to get to ILO Management screen.</li>
<li>Click <strong>User</strong> in the menu-bar and select <strong>Edit</strong>. Edit User pop-up box displays .</li>
<li>If you are using a BL server in the QA C7000 enclosure, select the <strong>cdl</strong> user to edit.</li>
<li>Use ↓(down arrow key) to select <strong>Administer User Accounts</strong>. </li>
<li>Use the space bar to set the value to <strong>YES</strong>.</li>
<li>Select <strong>F10</strong> to save.</li>
<li>Click <strong>File</strong> and select <strong>Exit</strong> to close.
<br /><br /></li>
</ol><hr /><h3 id="failure-update-overcloud">Failure of Update overcloud</h3>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>Update overcloud fails with the following error:</p>

<p>
  <code>Inconsistency between heat description ($OVERCLOUD_NODES) and overcloud configuration ($OVERCLOUD_INSTANCES)</code>
</p>

<p>
  <strong>Resolution</strong>
</p>

<p>If you get this error, perform the below steps:</p>

<ol><li><p>Log in to Seed.</p>

<pre><code># ssh root@&lt;Seed IP address&gt;
</code></pre>

<ol><li>Edit <code>/root/tripleo/ce_env.json</code>and update the right variable for build_number and installed_build_number. <!-- (CORE-1697) --></li>
</ol></li>
</ol><p>The ce_env_json will be displayed as the sample below.</p>

<pre>
  <code>      "host-ip": "192.168.122.1", 
       "hp": { 
         "build_number": 11, 
         "installed_build_number": 11 
</code>
</pre>

<p>Note that  the build_number is changed from null to the right variable.</p>

<p>3.Run the installer script to update the overcloud.</p>

<pre>
  <code>    # bash -x tripleo/tripleo-incubator/scripts/hp_ced_installer.sh --update-overcloud |&amp; tee update_cloud.log
</code>
</pre>

<p>During the installation, the number of build_number and installed_build_number that you specified are installed.
<br /></p>

<hr /><h3 id="installation-failure">Installation failure as the flavor to be used for overcloud nodes does not match</h3>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>If you have a set of Baremetal servers which differ in specifications (e.g. memory and disk), the installation will fail as the flavor to be used for overcloud nodes does not match with the server that has the lowest specification for memory, disk, and CPU.</p>

<p>
  <strong>Probable Cause</strong>
</p>

<p>The 2nd row in <code>baremetal.csv</code> which corresponds to the overcloud Controller node is used to create a flavor for the overcloud nodes.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>Edit the <strong>baremetal.csv</strong> file to define the lowest specification server in the second row.
<br /><br /></p>

<hr /><h4 id="PXE-boot-on-target">PXE boot on target node keeps switching between interfaces</h4>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>When node boots up on iLO console it shows node waiting for PXE boot on multiple NICs.</p>

<p>
  <strong>Probable Cause</strong>
</p>

<p>Multiple NICs are enabled for Network Boot.</p>

<p>
  <strong>Resolution</strong>
</p>

<ul><li>Reboot the node, using <strong>F9</strong> to get to the BIOS configuration.</li>
<li>Assuming NIC1(eth0/em1) for the node is connected to a private network shared across node enable it for Network Boot.</li>
<li>Select System Options &gt; Embedded NICs.</li>
<li>Set NIC 1 Boot Options = Network Boot.</li>
<li>Set NIC 2 Boot Options = Disabled.
<br /><br /></li>
</ul><hr /><h4 id="bios-blocks-are-not-set-to-correct-date">BIOS blocks are not set to correct date and time across all nodes</h4>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>Nodes PXE boot but ISCSI does not start.</p>

<p>
  <strong>Probable Cause</strong>
</p>

<p>Time and date across nodes are incorrect.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>Reboot the node, using <strong>F9</strong> to get to the BIOS configuration. BIOS date and time are set correctly and the same on all the systems.</p>

<ul><li>Select Date and Time.</li>
<li>Set the Date.</li>
<li>Set the Time.</li>
<li>Use the &lt;ENTER&gt; key to accept the new date and time.</li>
<li>Save the BIOS, which reboots the node again.</li>
<li>Once the node has rebooted, you can confirm its data and time from the iLO Overview.
<br /><br /></li>
</ul><hr /><h4 id="ilo-console">iLO console shows hLinux daemon.err tgtd while PXE booting</h4>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>PXE boot gets stuck after <code>daemon.err tgtd</code></p>

<p>
  <strong>Probable Cause</strong>
</p>

<p>Node does not have enough disk space</p>

<p>
  <strong>Resolution</strong>
</p>

<ul><li>Check if target node has disk space mentioned in <code>baremetal.csv</code> and is greater than Node_min_disk mentioned in <code>tripleo/tripleo-incubator/scripts/hp_ced_functions.sh</code>.</li>
<li>If disk space is less than Node_min_disk, change Node_min_disk along with DISK_SIZE in <code>tripleo/tripleo-incubator/scripts/hp_ced_list_nodes.sh</code> on Seed.</li>
<li>Re-run the installation script.
<br /></li>
</ul><hr /><h4 id="ilo-show-null">iLO console shows null waiting for notice of completion while PXE booting</h4>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>Node is powered on and PXE booted but it is powered off after <code>daemon.err</code> and stack create fails.</p>

<p>
  <strong>Probable Cause</strong>
</p>

<p>Node does not have enough disk space. SAN boot is enabled for node or local disk is not attached to <code>/sda</code></p>

<p>
  <strong>Resolution</strong>
</p>

<p>Installer expects that SAN boot option is disabled for nodes. Verify whether SAN boot is disabled for BL 490c.</p>

<p>Also, you can boot the targeted BL490c with Ubuntu or any Linux ISO to see what device is shown as the local disk. For the installer it should be <code>/sda</code>.</p>

<p>
  <br />
</p>

<hr /><h4 id="failure-installer">Failure of Hp_ced_installer.sh</h4>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p><code>Hp_ced_installer.sh</code> fails because of <code>baremetal.csv /sda</code>.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>Verify <code>baremetal.csv</code> for empty lines or special characters.
<br /><br /></p>

<hr /><h4 id="seed-install-failure">Failure of Seed Installation</h4>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>Seed installation fails with no space left on device.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>Verify the tripleo directory- user owner and group. It must be <strong>root:root</strong>. Incase it is not set as <strong>root:root</strong> then edit it to root using- <code>chown root:root tripleo</code></p>

<p>
  <br />
  <br />
</p>

<hr /><p>
  <strong>System Behavior/Message</strong>
</p>

<p>Inconsistent Rabbitmq failure seen on controller nodes while listing queues</p>

<pre>
  <code>rabbitmqctl list_queues
</code>
</pre>

<p>
  <strong>Resolution</strong>
</p>

<p>Restart the Rabbitmq service.</p>

<p>
  <br />
  <br />
</p>

<hr /><h2 id="ironic">Ironic intermitently set maintenance mode to True during installation</h2>

<p>This issue can happen during the update of undercloud or overcloud nodes. The update will fail for one or more nodes. <!-- CORE-2082 --></p>

<p>
  <strong>System Behavior/Message:</strong>
</p>

<p>If the update fails, from undercloud node:</p>

<ol><li><p>Source the stackrc file:</p>

<pre><code>source stackrc 
</code></pre></li>
<li><p>Execute the <code>nova list</code> command to determine which Compute node(s) is in an error state. The node will have a status of ERROR.</p>

<pre><code>nova list
</code></pre></li>
<li><p>Execute the <code>heat stack-list</code> command to determine which Heat stack is in an error state. The stack will have a status of <code>CREATE_FAILED</code>.</p>

<pre><code>heat stack-list
</code></pre></li>
<li><p>Execute the <code>ironic node-list</code> command to determine which node(s) is in maintenance mode. The stack will have a maintenance of <code>TRUE</code>.</p>

<pre><code>ironic node-list
</code></pre></li>
<li><p>Execute the <code>ironic node-show</code> command for the node that is node(s) is in maintenance mode. The stack will have a maintenance of <code>TRUE</code>.</p>

<pre><code>ironic node-show &lt;UUID&gt;
</code></pre>

<p>In the output, check the <code>last_error</code> field for an error similar to the following:</p>

<pre><code>During sync_power_state, max retries exceeded for node 81baacd5-657e-476f-b7ef, node state None does not match expected state

'None'. Updating DB state to 'None' Switching node to maintenance mode. 
</code></pre></li>
</ol><p>
  <strong>Solution</strong>
</p>

<ol><li><p>Remove the node in maintenance mode using the following command:</p>

<pre><code>nova node-delete &lt;ID of error node&gt;
</code></pre></li>
<li><p>List the stacks using the following command:</p>

<pre><code>heat stack-list
</code></pre></li>
<li><p>Delete the stack with the failed Nova node.</p>

<pre><code>heat stack-delete &lt;ID of failed node&gt;
</code></pre></li>
<li><p>Change the node(s) to false for the maintenance option, using the following command:</p>

<pre><code>`ironic node-update &lt;id&gt; replace maintenance=False`
</code></pre></li>
</ol><h2 id="esx-ovsvapp">ESX and OVSvAPP</h2>

<ol><li><a href="#nova-compute">nova-manage service list does not list the compute service as running</a></li>
<li><a href="#unable-login-vcenter">Unable to login to vCenter proxy agent</a></li>
<li><a href="#unable-cinder-backup">Unable to backup volumes using Cinder backup</a></li>
<li><a href="#fails-ovsvapp">Failure of OVSvAPP deployment</a></li>
</ol><h3 id="nova-compute">nova-manage service list does not list the compute service as running</h3>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>There can be multiple reason why nova-compute service is not listed or has a :) as status.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>To resolve the above issue verify the following:</p>

<ol><li>The ESX Management Network is able to reach the Helion Management Network.</li>
<li>nova-compute service is running (os-svc-restart -n nova-compute).</li>
<li>Verify <code>/etc/nova/nova-compute.conf</code> has the right entries.
<br /></li>
</ol><hr /><h3 id="unable-login-vcenter">Unable to login to vCenter proxy agent</h3>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>Unable to login to vCenter proxy agent through the console.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>Users can login to the system using the user <code>heat-admin</code> and the authorized key in the Seed VM.
<br /><br /></p>

<hr /><h3 id="unable-cinder-backup">Unable to backup volumes using Cinder backup</h3>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>Unable to backup volumes using Cinder backup.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>Cinder-backup is not supported.
<br /><br /></p>

<hr /><h3 id="fails-ovsvapp">Failure of OVSvAPP deployment</h3>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>Failure of OVSvAPP deployment.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>Verify <code>tripleo/hp-ovsvapp/log/ovs_vapp.log</code> in the installer directory.</p>

<p>
  <br />
  <br />
</p>

<hr /><p>
  <strong>System Behavior/Message</strong>
</p>

<p>After reboot of Controller that has the VIP assigned, the hpvcn agent, nova-compute service, nova compute service in the proxy node and HCN agent in OVSvAPP needs to be restarted manually to resume normal operations.</p>

<p>
  <strong>Resolution</strong>
</p>

<ul><li><p>To restart nova-compute, execute the following command in compute proxies</p>

<pre><code># service nova-compute restart  
</code></pre></li>
<li><p>To restart HP VCN agent, execute the following command in OVSvAPP vm's</p>

<pre><code>#service hpvcn-neutron-agent restart 
</code></pre>

<p><br /><br /></p></li>
</ul><hr /><h2 id="vsa">VSA</h2>

<ol><li><a href="#fails-retrieve-netmask">Failure to retrieve netmask from vsa-bridge</a></li>
<li><a href="install-script-detect.html">Installation script detects more than 7 available drive</a></li>
<li><a href="#failure-script">Failure of script due to less than two drives</a></li>
<li><a href="#cannot-enable-ao">Cannot enable AO as only one disk is available</a></li>
<li><a href="#unable-update-json">Unable to update the default input json file </a></li>
<li><a href="#fail-virtual-bridge">Virtual bridge creation failed for interface <nic></nic></a></li>
<li><a href="#storage-pool-fail">Creation of storage pool failed</a></li>
<li><a href="#post-vsa-fail">Failed during post VSA deployment</a></li>
<li><a href="#vsa-network">vsa_network cannot be destroyed</a></li>
<li><a href="#vsa-pool-cannot-destroy">vsa_storage_pool pool cannot be destroyed</a></li>
</ol><h3 id="fails-retrieve-netmask">Failure to retrieve netmask from vsa-bridge</h3>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>Cannot retrieve netmask from interface vsa-bridge</p>

<p>
  <strong>Probable Cause</strong>
</p>

<p>VSA deployment script determines the net-mask and gateway details from the provided interface. When there is no IP address assigned to the VSA bridge, this error may occur.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>To resolve this issue, perform the following steps:</p>

<ul><li><p>Check whether the IP address is allocated for the VSA bridge</p></li>
<li><p>Verify the VSA IP address by using the following command:</p>

<pre><code>ifconfig vsa-bridge
</code></pre></li>
</ul><p>
  <br />
</p>

<hr /><h3 id="install-script-detect">Installation script detects more than 7 available drive</h3>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>Maximum supported devices 7.</p>

<p>
  <strong>Probable Cause</strong>
</p>

<p>This issue occurs when there are more than 7 available drives detected by the installation script to deploy StoreVirtual.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>Perform the following steps:</p>

<ul><li><p>HP StoreVirtual VSA supports up to 7 disks</p></li>
<li><p>Execute <code>fdisk &amp;#45;l</code> and check for number of available drives in the machine other than <code>/dev/sda</code></p></li>
</ul><p>
  <br />
</p>

<hr /><h3 id="failure-script">Failure of script due to less than two drives</h3>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>Minimum number of disks must be 2. No disks are available.</p>

<p><strong>Probable Cause</strong>
When there are less than two drives in the machine, the script will fail to execute.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>To resolve, perform the following steps:</p>

<ul><li><p>Execute <code>fdisk &amp;#45;l</code></p></li>
<li><p>Minimum two drives and maximum of 7 drives should be available for the StoreVirtual deployment other than boot disk(<code>/dev/sda</code>)</p></li>
<li><p>At least three drives required for enabling AO
<br /></p></li>
</ul><hr /><h3 id="cannot-enable-ao">Cannot enable AO as only one disk is available</h3>

<p>
  <strong>Probable Cause</strong>
</p>

<p>For Adaptive Optimization to be enabled, at least three drives must be available. <code>/dev/sdb</code> must be SSD drive(Tier 0) and the remaining will be Tier 1.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>To resolve the issue, do the following:</p>

<ul><li><p>Use RAID controllers to create RAID groups.</p></li>
<li><p>Ensure that you create the RAID group for SSD drives immediately after creating the RAID group for boot volume. For example: If three RAID groups are to be created. The following is recommended :</p>

<ul><li><p><strong>Step 1</strong> : Create the first RAID group for HDD drives and mark this as boot volume(/dev/sda)</p></li>
<li><p><strong>Step 2</strong>: Create the second RAID group for SSD drives which should be used as Tier 0 for AO (/dev/sdb)</p></li>
<li><p><strong>Step 3</strong>: Create the third RAID group for HDD drives which will be used as Tier 1(/dev/sdc)</p></li>
</ul></li>
</ul><p>
  <br />
</p>

<hr /><h3 id="unable-update-json">Unable to update the default input json file</h3>

<p>
  <strong>System Behavior/Message</strong>
</p>

<p>Parsing the default JSON file failed. Unable to update the default input json file.</p>

<p>
  <strong>Probable Cause</strong>
</p>

<p>The script will parse the configuration file and update the values based on the network and configuration files.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>Perform the following steps:</p>

<ul><li><p>Verify whether the JSON content is valid in the following files:</p>

<ul><li><p><code>/home/vsa-installer/pyVins/etc/vsa/vsa_config.json</code></p></li>
<li><p><code>/etc/vsa/vsa_network_config.json</code></p></li>
</ul></li>
</ul><p>
  <br />
</p>

<hr /><h3 id="storage-pool-fail">Creation of storage pool failed</h3>

<p>
  <strong>Probable Cause</strong>
</p>

<p>Virtual storage pool will be created for placing the extracted VSA VM image. The storage pool will be created based on local directory  on <code>/mnt/state/vsa-kvm-storage</code></p>

<p>
  <strong>Resolution</strong>
</p>

<p>Perform the following steps:</p>

<ul><li><p>Check whether <code>/mnt/state/vsa-kvm-storage</code> directory is available.</p></li>
<li><p>Verify for available space to create storage pool in the system.</p></li>
<li><p>Check the libvirt logs for more errors</p></li>
</ul><p>Refer <code>/var/log/libvirt/libvirt.log</code> on VSA system.</p>

<p>
  <br />
</p>

<hr /><h3 id="post-vsa-fail">Failed during post VSA deployment</h3>

<p>
  <strong>Probable Cause</strong>
</p>

<p>The script will persist required files in <code>/mnt/state/vsa</code> which will be used for recreating the VSA VM during re-imaging scenario</p>

<p>
  <strong>Resolution</strong>
</p>

<p>This error will occur if the script fails to find <code>network_vsa.xml</code>, <code>storagepool_vsa.xml</code> and other configuration files which has to be preserved.</p>

<ul><li><p>Check for the configuration files on ”/‘ path.</p></li>
<li><p>On success, the script updates the <code>/mnt/state/vsa/vsa_config.json</code> file with the updated and created time.</p></li>
</ul><p>
  <br />
</p>

<hr /><h3 id="vsa-install-fail">VSA installation failed</h3>

<p>
  <strong>Probable Cause</strong>
</p>

<p>When VSA installation fails for any of the above reasons, the script will rollback the network and storage pool.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>Verify the <code>/installer.log</code></p>

<p>
  <br />
</p>

<hr /><h3 id="vsa-network">vsa_network cannot be destroyed</h3>

<p>
  <strong>Probable Cause</strong>
</p>

<p>VSA network will be destroyed when the VSA installation fails.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>Perform the following steps:</p>

<ul><li><p>Check whether the network is already undefined</p></li>
<li><p>Check whether the network name in <code>&lt;PYVINS_DIRS&gt;/etc/vsa/vsa_config.json</code> is the same as in the output of <code>virsh net-list -all</code> command</p></li>
</ul><p>
  <br />
</p>

<hr /><h3 id="vsa-pool-cannot-destroy">vsa_storage_pool pool cannot be destroyed</h3>

<p>
  <strong>Probable Cause</strong>
</p>

<p>The storage pool will be destroyed when VSA installation fails</p>

<p>
  <strong>Resolution</strong>
</p>

<p>Perform the following:</p>

<ul><li><p>Verify whether the storage pool is already undefined</p></li>
<li><p>Verify whether the pool name is same as in <code>&lt;PYVINS_DIRS&gt;/etc/vsa/vsa_config.json</code></p></li>
<li><p>Virsh command to list the pools</p>

<pre><code>Virsh pool-list --all
</code></pre></li>
</ul><p>
  <br />
</p>

<hr /><!---
##Configuring the dnsmasq_dns_servers list for the undercloud and overcloud {#config_dnas}
=======
## Failure of newly added compute or VSA node during Scale-out {#failnew}

**System Behavior/Message**

The newly added compute node or VSA node fails during scale-out.

**Resolution**

You must remove a failed compute or VSA node before adding a new compute node.

Perform the following steps to remove a failed compute node:

1. Run `heat stacklist`on the undercloud node and search for the failed stack.

2. Delete the failed stack using the following command:

        # heat stackdelete <stackname or uuid>

3. List the newly added nova node which is created during scale-out.

        # novalist

4. Execute the following command to delete nova node. Node name and the Node ID can be obtained from the above steps.

        # nova delete <node name or node id>

5. View a newly added node using the following command:

        # ironic nodelist

6. If newly added node is in ERROR state or if the maintenance mode is True then remove those node(s) using the following command.

        # ironic nodedelete <uuid>

    where uuid is the ID of the node

<HR>

## Scale-out nodes : os-refresh-configuration fails on Controller Nodes {#refreshfails}

**System Behavior/Message**

The os-refresh-config on the controller Nodes fail during scale-out.

**Probable Cause**

The controller nodes can fail due to following reasons:

* RabbitMQ clustering
* MySQL clustering

**Resolution**

To resolve RabbitMQ cluster issue:

* Use the following command and verify the running status of RabbitMQ.

    status rabbitmqserver

If RabbitMQ is not running, start RabbitMQ using the `start rabbitmqserver` command.

* Verify that the `rabbitmqctl cluster_status` displays all 3 nodes in `running_nodes` and disc. If it does not display one or more nodes in running nodes then restart RabbitMQ and run the following command on the missing nodes:

    rabbitmqctl join_cluster <clusternode>

* If rabbitmqctl cluster_status displays expected output but there is an issue with one or more node(s) for joining RabbitMQ cluster, do the following:

    1. Execute the following commands on all controller nodes:

            pkill u rabbitmq

    2. Run `osrefreshconfig` command first on the `cluster_name` (`rabbitmqctl cluster_status` output) and then on the remaining controller nodes.

**To resolve MySQL cluster issue**

1. Use the following command and verify the running status of MySQL on the node.

        /etc/init.d/mysql status

    If mysql has stopped, restart it.

2. If MySQL fails to restart, perform the following instructions:

* Run mysqld_safe wsreprecover on all the controller nodes.
* Compare the output from all controller nodes for last committed transaction sequence number. For example:

        root@overcloudcecontrollercontroller0defen5afl75f:~#
        mysqld_safe wsreprecover
        sed: -e expression #1, char 25: unknown option to `s'
        sed: -e expression #1, char 24: unknown option to `s'
        141113 01:00:36 mysqld_safe Logging to '/mnt/state/var/log/mysql/error.log'.
        141113 01:00:36 mysqld_safe Starting mysqld daemon with databases from /mnt/state/var
        141113 01:00:36 mysqld_safe Skipping wsreprecover for 1e9d939a6a0711e49c28aa3122
        141113 01:00:36 mysqld_safe Assigning 1e9d939a6a0711e49c28aa31223485e0:220764 to 141113 01:00:38 mysqld_safe mysqld from pid file /var/run/mysqld/mysqld.pid ended 

    So the last committed transaction sequence number on this node is 220764.

* Compare the last committed transaction sequence number across all 3 nodes and bootstrap from the latest node using `/etc/init.d/mysql bootstrappxc` or `/etc/init.d/mysql restart` and start MySQL on the remaining nodes.

<HR>

##Configuring the `dnsmasq_dns_servers` list for the undercloud and overcloud {#config_dnas}
>>>>>>> b9d57eb337ad163596659fb19b8dbd01770325a1

To enable name resolution from tenant VMs in the overcloud, it is necessary
to configure the DNS servers which will be used by `dnsmasq` as forwarders.  To
perform this:

1. Edit the `overcloud_neutron_dhcp_agent.json` file in the
`ce-installer/tripleo/hp_passthrough` directory to add the desired `dnsmasq_dns_servers`
items. 
2. Copy the `overcloud_neutron_dhcp_agent.json` file to a
new file named `undercloud_neutron_dhcp_agent.json` and configure the same
forwarders for the undercloud.

    
        {"dhcp_agent":
          {"config":
            [
              {"section":"DEFAULT",
                "values":
                  [
                    {"option":"dhcp_delete_namespaces","value":"True"},
                    {"option":"dnsmasq_dns_servers", "value":"0.0.0.0"}  <----set the value to the ip
                                                                              address of the DNS server
                                                                              to use.  Multiple DNS
                                                                              servers can be specified
                                                                              as a comma separated list.
                  ]
              }
            ]
          }
        }

<hr>
----><h2 id="recovery-when-scale-out-nodes-of-newly-added-compute-node-or-vsa">Recovery when Scale-out nodes of newly added compute node or VSA</h2>

<p>
  <strong>
    <em>System Behavior/Message</em>
  </strong>
</p>

<p>The newly added compute node or VSA node fails during scale-out.</p>

<p>
  <strong>Resolution</strong>
</p>

<p>You must remove a failed compute node before adding a new compute node.</p>

<p>Perform the following steps to remove a failed compute node:</p>

<ol><li>Run <code>heat stack-list</code> on the undercloud node and search for failed stack.</li>
<li><p>Delete the failed stack using the following command:</p>

<pre><code># heat stack-delete &lt;stackname or uuid&gt;
</code></pre></li>
<li><p>List the newly added nova node which is created during scale-out.</p>

<pre><code># nova-list
</code></pre></li>
<li><p>Execute the following command to delete nova node. Node name and ID is obtained from step 3.</p>

<pre><code># nova delete &lt;name or id&gt;
</code></pre></li>
<li><p>View a newly added node using the following command:</p>

<pre><code># ironic node-list
</code></pre></li>
<li><p>If newly added node is in <strong>ERROR</strong> state or it has maintenance as <strong>True</strong> then remove those node(s) using following command.</p>

<pre><code># ironic node-delete &lt;uuid&gt;, where uuid is the ID of the node
</code></pre></li>
</ol><hr /><h2 id="scale-out-nodes--os-refresh-config-on-controller-nodes-fail">Scale-out nodes : os-refresh-config on Controller Nodes Fail</h2>

<p>
  <strong>
    <em>System Behavior/Message</em>
  </strong>
</p>

<p>The os-refresh-config on controller Nodes fail during scale-out.</p>

<p>
  <strong>
    <em>Probable Cause</em>
  </strong>
</p>

<p>The controller nodes can fail due to following reasons:</p>

<ul><li><p>wrong user input</p></li>
<li><p>rabbitmq clustering</p></li>
<li><p>mysql clustering</p></li>
</ul><p>
  <strong>Resolution</strong>
</p>

<p>
  <strong>To resolve rabbitmq cluster issue</strong>
</p>

<ul><li><p>Use the following command and verify the running status of rabbitmq.</p>

<pre><code>status rabbitmq-server 
</code></pre>

<p>If rabbitmq is not running, start rabbitmq  using <code>start rabbitmq-server</code> command.</p></li>
<li><p>Verify that the <code>rabbitmqctl cluster_status</code> displays all 3 nodes in <code>running_nodes</code>, disc. If it does not display one or more nodes in running nodes then restart rabbitmq and run the following command on the missing nodes:</p>

<pre><code>rabbitmqctl join_cluster &lt;clusternode&gt;
</code></pre></li>
<li><p>If <code>rabbitmqctl cluster_status</code> shows expected output but there is an issue with one or more node(s) for joining rabbitmq cluster, do the following:</p>

<ol><li><p>Execute the following commands on all controller nodes:</p>

<pre><code>pkill -u rabbitmq  
</code></pre></li>
<li><p>Run <code>os-refresh-config</code>  command first on the <code>cluster_name</code> (rabbitmqctl cluster_status output) and on the remaining controller nodes.</p></li>
</ol></li>
</ul><p>
  <strong>Resolve mysql cluster issue</strong>
</p>

<ol><li><p>Use the following command and verify the running status of mysql on the node.</p>

<pre><code>/etc/init.d/mysql status
</code></pre>

<p>If mysql has stopped, restart it.</p></li>
<li><p>If mysql fails to restart, perform the following instructions:</p>

<ul><li>Run  <code>mysqld_safe --wsrep-recover</code> on all controller nodes.</li>
<li><p>Compare the output from all controller nodes for last committed transaction sequence number. For example:</p>

<pre><code>root@overcloud-ce-controller-controller0-defen5afl75f:~# mysqld_safe --wsrep-recover
sed: -e expression #1, char 25: unknown option to `s'
sed: -e expression #1, char 24: unknown option to `s'
141113 01:00:36 mysqld_safe Logging to '/mnt/state/var/log/mysql/error.log'.
141113 01:00:36 mysqld_safe Starting mysqld daemon with databases from /mnt/state/var/lib/mysql/
141113 01:00:36 mysqld_safe Skipping wsrep-recover for 1e9d939a-6a07-11e4-9c28-aa31223485e0:220764 pair
141113 01:00:36 mysqld_safe Assigning 1e9d939a-6a07-11e4-9c28-aa31223485e0:220764 to wsrep_start_position
141113 01:00:38 mysqld_safe mysqld from pid file /var/run/mysqld/mysqld.pid ended
</code></pre></li>
</ul><p>So the last committed transaction sequence number on this node is 220764.</p>

<ul><li>Compare the last committed transaction sequence number across all 3 nodes and bootstrap from the latest node using <code>/etc/init.d/mysql bootstrap-pxc</code> or <code>/etc/init.d/mysql restart</code> and start mysql on the remaining nodes.</li>
</ul></li>
</ol><h2 id="logging">Logging</h2>

<h4 id="issue-in-logging">Issue in Logging</h4>

<p>The user needs to manually follow the below steps to re-configure Kibana for logging.</p>

<ol><li>Log in to the undercloud and start screen session.</li>
<li>In the screen, start following command <code>sudo -u logstash /usr/bin/java -Xmx1g -Djava.io.tmpdir=/var/lib/logstash/ -jar /opt/logstash/logstash.jar agent -f /etc/logstash/conf.d -w 10 --log /var/log/logstash/logstash.log</code></li>
<li>Press Control <strong>&amp;</strong> '<strong>a</strong>', then '<strong>c</strong>' to create another shell.</li>
<li>In a new shell execute command <code>sudo -u logstash /usr/bin/java -Xmx1g -Djava.io.tmpdir=/var/lib/logstash/ -jar /opt/logstash/logstash.jar agent -f /etc/logstash/conf.d -w 10 --log /var/log/logstash/logstash.log</code></li>
<li>Repeat steps from <strong>3-4</strong> two times</li>
<li>Press Control <strong>&amp;</strong> '<strong>a</strong>' then '<strong>d</strong>' to detach.</li>
</ol><p><strong>Note</strong>: If node reboots repeat the step from <strong>1-6</strong>.</p>

<p><strong>EDIT</strong>: Added <code>sudo -u logstash</code> at beginning of commands.</p>

<p>
  <a href="#top" style="padding:14px 0px 14px 0px; text-decoration: none;"> Return to Top ↑</a>
</p>

<hr />
	</article>
</div>

<div id="sidebar" class="grid_5 top-spacer">
	<aside class="signup-box">
		<p class="align-center">
			Start using HP Cloud<br>
			<a href="../../../../../external.html?link=https://helion.hpwsportal.com/#/Home/Show" class="signup-button-orange gutter-top-half">Sign Up Now</a>
		</p>
	</aside>
	<aside class="box section-navigation">
  
     
  
  <ul>
    
      
      
      <li>
        <a class="" href="../../../../index.html"><span>HP Helion Documentation</span></a>
      </li>
    
      
      
      <li>
        <a class="" href="../../index.html"><span>HP Helion OpenStack Documentation Home</span></a>
      </li>
    
      
      
      <li>
        <a class="" href="../../release-notes/101/index.html"><span>Release Notes</span></a>
      </li>
    
      
      
      <li>
        <a class="" href="../../faq/index.html"><span>FAQ</span></a>
      </li>
    
      
      
      <li>
        <a class="" href="../../technical-overview/index.html"><span>Technical Overview</span></a>
      </li>
    
      
      
      <li>
        <a class="" href="../overview/index.html"><span>Services Overview</span></a>
      </li>
    
      
      
      <li>
        <a class="" href="../../support-matrix/index.html"><span>Support Matrix</span></a>
      </li>
    
      
      
      <li>
        <a class="" href="../../install/overview/index.html"><span>Installation Overview</span></a>
      </li>
    
      
      
      <li>
        <a class="" href="../../install/security/index.html"><span>Network Security</span></a>
      </li>
    
      
      
      <li>
        <a class="" href="../../backup.restore/index.html"><span>Back Up and Restore</span></a>
      </li>
    
      
      
        
      
      <li>
        <a class="highlighted" href="index.html"><span>Troubleshooting</span></a>
      </li>
    
      
      
      <li>
        <a class="" href="../../related-links/index.html"><span>Related Documentation</span></a>
      </li>
    
      
      
      <li>
        <a class="" href="../../glossary/index.html"><span>Glossary</span></a>
      </li>
    
      
      
      <li>
        <a class="" href="../../eula/index.html"><span>End User License Agreement</span></a>
      </li>
    
      
      
      <li>
        <a class="" href="../../3rd-party-license-agreements/index.html"><span>Open Source and Third-Party Software License Agreements</span></a>
      </li>
    
      
      
      <li>
        <a class="" href="../../siteindex/index.html"><span>Site Index</span></a>
      </li>
    
  </ul>
</aside>
</div>

			</div> <!-- #main -->
		</div> <!-- #main-container -->
	</div> <!-- #wrapper -->

		
  <footer class="footer">
    <div class="container_16">
    	<div class="wrapper grid_16">
    		<ul class="footer-social">
    			<li class="twitter"><a href="../../../../../external.html?link=https://twitter.com/hphelioncloud"></a></li>
    			<li class="youtube"><a href="../../../../../external.html?link=http://www.youtube.com/hpcloud"></a></li>
    			<li class="rss"><a href="../../../../../external.html?link=http://h30499.www3.hp.com/hpeb/rss/board?board.id=sws-661"></a></li>
    			<li class="facebook"><a href="../../../../../external.html?link=https://www.facebook.com/HPCloud"></a></li>
    			<li class="linkedin stretch"><a href="../../../../../external.html?link=https://www.linkedin.com/company/hewlett-packard"></a></li>
    		</ul>
    		<ul class="contact-info">
    			<li>In the U.S.<br />1-855-61CLOUD</li>
    			<li>Worldwide<br />+1678-745-9010</li>
    			<li class="col-3">Online Support<br /><a href="mailto:support@hpcloud.com">Email</a> 
				<!-- comment out per JC
				or <span class="link" onclick="jQuery('#hpcloud-chat-button a').click();">Chat with us</span>
				-->
				</li>
    		</ul>
    		<div class="copyright">
    			<p>The OpenStack&reg; Word Mark and OpenStack Logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.</p><br />
    			<p>&copy; 2014 Hewlett-Packard Development Company, L.P.<br /><a href="../../../../../external.html?link=http://www.hpcloud.com/legal-documents">HP Cloud legal documents and privacy policy</a></p>
    		</div>
    	</div>
    </div>
  </footer>

	<script src="../../../../../ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
    <script src="../../../../../ajax.googleapis.com/ajax/libs/jqueryui/1.10.2/jquery-ui.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../../../js/jquery-1.8.2f.min.html"><\/script>')</script>

  <script src="../../../../js/combined.js"></script>
  <script>
      __HP_CHAT_CONF = {};
      __HP_CHAT_CONF.sales = true;
      __HP_CHAT_CONF.support = true;
    </script>
	
    <script> //removed per JC
    //(function($, window) {
    //    $(window).load(function() {
    //    var sNew = document.createElement("script");
    //  sNew.async = true;
    //sNew.src = 'https://a248.e.akamai.net/cdn.hpcloudsvc.com/gcbb18d905d32174e6eb205943b0dea4a/prodae1/chat.js';
    //var s0 = document.getElementsByTagName('script')[0];
    //s0.parentNode.insertBefore(sNew, s0);
    //});
    //})(jQuery, this);
    </script>
    <script>
        //Executes your code when the DOM is ready.  Acts the same as $(document).ready().
        function styleRestCmds() {
            $("h5:contains('GET')").addClass('rest_cmd cmd_get');
            $("h5:contains('POST')").addClass('rest_cmd cmd_post');
            $("h5:contains('FormPOST')").removeClass('rest_cmd cmd_post');
            $("h5:contains('PUT')").addClass('rest_cmd cmd_put');
            $("h5:contains('COPY')").addClass('rest_cmd cmd_copy');
            $("h5:contains('PATCH')").addClass('rest_cmd cmd_patch');
            $("h5:contains('HEAD')").addClass('rest_cmd cmd_head');
            $("h5:contains('DELETE')").addClass('rest_cmd cmd_delete');
        }

        $(function() {
            //Calls the tocify method on your HTML div.
            $("#toc").tocify({
                'selectors': 'h2,h3,h4',
                'showAndHideOnScroll': false,
                'hideEffect': 'slideUp',
                'scrollTo': 55    // Amount of space between top of page and selected content
              }
            );
            // Style the REST commands
            styleRestCmds();
        });
    </script>
</body>

<!-- Mirrored from docs.hpcloud.com/helion/openstack/services/troubleshooting/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 20 Feb 2015 14:52:29 GMT -->
</html>
